{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined contents of ['01', '02', '03'] into C:/Users/LENOVO/Downloads/KArSL-502\\combined\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def create_combined_folder(base_path, combined_folder_name):\n",
    "    combined_path = os.path.join(base_path, combined_folder_name)\n",
    "    if not os.path.exists(combined_path):\n",
    "        os.makedirs(combined_path)\n",
    "    return combined_path\n",
    "\n",
    "def copy_contents(src, dst):\n",
    "    for root, dirs, files in os.walk(src):\n",
    "        relative_path = os.path.relpath(root, src)\n",
    "        target_dir = os.path.join(dst, relative_path)\n",
    "        if not os.path.exists(target_dir):\n",
    "            os.makedirs(target_dir)\n",
    "        for file in files:\n",
    "            shutil.copy2(os.path.join(root, file), os.path.join(target_dir, file))\n",
    "\n",
    "def main():\n",
    "    base_path = 'C:/Users/LENOVO/Downloads/KArSL-502'\n",
    "    folders_to_combine = ['01', '02', '03']\n",
    "    combined_folder_name = 'combined'\n",
    "\n",
    "    combined_path = create_combined_folder(base_path, combined_folder_name)\n",
    "\n",
    "    for folder in folders_to_combine:\n",
    "        src_path = os.path.join(base_path, folder)\n",
    "        copy_contents(src_path, combined_path)\n",
    "\n",
    "    print(f\"Combined contents of {folders_to_combine} into {combined_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Path to the directory containing the folders\n",
    "directory_path = 'Data/train'\n",
    "\n",
    "# List all the folders in the directory\n",
    "folders = os.listdir(directory_path)\n",
    "\n",
    "for folder in folders:\n",
    "    # Check if the folder name starts with '0'\n",
    "    if folder.startswith('0'):\n",
    "        # New folder name without the leading zero\n",
    "        new_folder_name = folder[1:]\n",
    "        # Old folder path\n",
    "        old_folder_path = os.path.join(directory_path, folder)\n",
    "        # New folder path\n",
    "        new_folder_path = os.path.join(directory_path, new_folder_name)\n",
    "        # Rename the folder\n",
    "        os.rename(old_folder_path, new_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def assign_labels(files_path, df):\n",
    "    data_list = []\n",
    "\n",
    "    folders = os.listdir(files_path)\n",
    "\n",
    "    for i, j in enumerate(folders):\n",
    "        folder_sign_number = int(j)\n",
    "        \n",
    "        if folder_sign_number in df['Sign Number'].values:\n",
    "            row = df[df['Sign Number'] == folder_sign_number].iloc[0]\n",
    "            folder_path = os.path.join(files_path, j)\n",
    "            label = row['Arabic Sign']\n",
    "            \n",
    "            for file_name in os.listdir(folder_path):\n",
    "                file_path = os.path.join(folder_path, file_name)\n",
    "                \n",
    "                data_list.append({'file_path': file_path, 'Label': label})\n",
    "\n",
    "    data = pd.concat([pd.DataFrame(data_list)], ignore_index=True)\n",
    "    data = data.sample(frac=1).reset_index(drop=True)\n",
    "    data = pd.get_dummies(data, columns=['Label'])\n",
    "    x = data['file_path']\n",
    "    y = data.loc[:, data.columns != 'file_path']\n",
    "\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from torch_geometric.utils import add_self_loops, to_dense_adj\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(r'C:\\Users\\LENOVO\\Downloads\\KArSL-502\\train.csv')\n",
    "test_df = pd.read_csv(r'C:\\Users\\LENOVO\\Downloads\\KArSL-502\\test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train_df['file_path'], train_df['Label']\n",
    "X_test, y_test = test_df['file_path'], test_df['Label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_frames(video_path, T=48):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    N = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    frames = []\n",
    "    if N < T:\n",
    "        # Read all frames if there are fewer than T frames\n",
    "        for i in range(N):\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                frames.append(frame)\n",
    "        # Interpolate frames to reach T frames\n",
    "        frames = interpolate_frames(frames, T)\n",
    "    else:\n",
    "        # Sample T frames evenly from the video\n",
    "        I = max(1, N // T)\n",
    "        for i in range(0, N, I):\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                frames.append(frame)\n",
    "            if len(frames) == T:\n",
    "                break\n",
    "    \n",
    "    cap.release()\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_frames(frames, T):\n",
    "    num_existing_frames = len(frames)\n",
    "    interpolated_frames = []\n",
    "    \n",
    "    for i in range(T):\n",
    "        t = i * (num_existing_frames - 1) / (T - 1)\n",
    "        lower_idx = int(np.floor(t))\n",
    "        upper_idx = int(np.ceil(t))\n",
    "        \n",
    "        if lower_idx == upper_idx:\n",
    "            interpolated_frames.append(frames[lower_idx])\n",
    "        else:\n",
    "            alpha = t - lower_idx\n",
    "            frame = cv2.addWeighted(frames[lower_idx], 1 - alpha, frames[upper_idx], alpha, 0)\n",
    "            interpolated_frames.append(frame)\n",
    "    \n",
    "    return interpolated_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_landmarks(frames):\n",
    "    mp_holistic = mp.solutions.holistic\n",
    "    landmarks_list = []\n",
    "    with mp_holistic.Holistic(static_image_mode=True) as holistic:\n",
    "        for frame in frames:\n",
    "            results = holistic.process(frame)\n",
    "            if results.pose_landmarks and results.left_hand_landmarks and results.right_hand_landmarks:\n",
    "                landmarks = []\n",
    "                pose_indices = [0, 11, 12, 13, 14]\n",
    "                for idx in pose_indices:\n",
    "                    lm = results.pose_landmarks.landmark[idx]\n",
    "                    landmarks.append((lm.x, lm.y, lm.z))\n",
    "                hand_indices = [0, 4, 5, 8, 9, 12, 13, 16, 17, 20]\n",
    "                for hand_landmarks in [results.left_hand_landmarks, results.right_hand_landmarks]:\n",
    "                    for idx in hand_indices:\n",
    "                        lm = hand_landmarks.landmark[idx]\n",
    "                        landmarks.append((lm.x, lm.y, lm.z))\n",
    "            else:\n",
    "                # Append default values if landmarks are not detected\n",
    "                landmarks = [(0.0, 0.0, 0.0)] * 25\n",
    "            landmarks_list.append(landmarks)\n",
    "    return landmarks_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_landmarks(landmarks_list):\n",
    "    normalized_landmarks_list = []\n",
    "    for landmarks in landmarks_list:\n",
    "        nose = landmarks[0] \n",
    "        normalized_landmarks = [(x - nose[0] + 1e-14, y - nose[1] + 1e-14, z - nose[2] + 1e-14) for x, y, z in landmarks]\n",
    "        normalized_landmarks_list.append(normalized_landmarks)\n",
    "    return normalized_landmarks_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_graph(normalized_landmarks_list):\n",
    "    G = nx.Graph()\n",
    "    T = len(normalized_landmarks_list)\n",
    "    X = []  # Node features matrix\n",
    "\n",
    "    # Define connections based on the landmarks\n",
    "    body_connections = [\n",
    "        # Upper Body\n",
    "        (0, 1), (0, 2), (1, 2), # Nose to shoulders\n",
    "        # Left Arm\n",
    "        (1, 3), # Shoulder to Elbow\n",
    "        # Right Arm\n",
    "        (2, 4), # Shoulder to Elbow\n",
    "    ]\n",
    "    \n",
    "    hand_connections = [\n",
    "        # Thumb\n",
    "        (1, 2), (2, 3), (3, 4),\n",
    "        # Index finger\n",
    "        (5, 6), (6, 7), (7, 8),\n",
    "        # Middle finger\n",
    "        (9, 10), (10, 11), (11, 12),\n",
    "        # Ring finger\n",
    "        (13, 14), (14, 15), (15, 16),\n",
    "        # Pinky finger\n",
    "        (17, 18), (18, 19), (19, 20),\n",
    "        # Wrist to fingers\n",
    "        (0, 1), (0, 5), (0, 9), (0, 13), (0, 17)\n",
    "    ]\n",
    "\n",
    "    for t in range(T):\n",
    "        for i, landmark in enumerate(normalized_landmarks_list[t]):\n",
    "            # Add node for each landmark in each frame with 3D position\n",
    "            G.add_node((t, i), pos=(landmark[0], landmark[1], landmark[2]))  \n",
    "            X.append([landmark[0], landmark[1], landmark[2]])  # Add to feature matrix\n",
    "            if t > 0:\n",
    "                # Connect the same landmark between consecutive frames (inter-frame edges)\n",
    "                G.add_edge((t-1, i), (t, i))\n",
    "        \n",
    "        # Add intra-frame edges based on the body and hand structure\n",
    "        for (i, j) in body_connections + hand_connections:\n",
    "            G.add_edge((t, i), (t, j))\n",
    "    \n",
    "    return G, np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_as_tensor(normalized_landmarks_list):\n",
    "    T = len(normalized_landmarks_list)\n",
    "    V = len(normalized_landmarks_list[0])\n",
    "    C = 3\n",
    "\n",
    "    tensor = np.zeros((C, V, T))\n",
    "\n",
    "    for t in range(T):\n",
    "        for v in range(V):\n",
    "            x, y, z = normalized_landmarks_list[t][v]\n",
    "            tensor[0, v, t] = x\n",
    "            tensor[1, v, t] = y\n",
    "            tensor[2, v, t] = z\n",
    "\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adjacency_matrix(graph):\n",
    "    adj_matrix = nx.adjacency_matrix(graph).todense()\n",
    "    #adj_matrix = np.linalg.inv(np.sqrt(adj_matrix))\n",
    "    return adj_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(video_path, label, T=48):\n",
    "    sampled_frames = sample_frames(video_path, T)\n",
    "    detected_landmarks = detect_landmarks(sampled_frames)\n",
    "    normalized_landmarks = normalize_landmarks(detected_landmarks)\n",
    "    graph, X = construct_graph(normalized_landmarks)\n",
    "    adj_matrix = get_adjacency_matrix(graph)\n",
    "    \n",
    "    edge_index, _ = dense_to_sparse(torch.tensor(adj_matrix, dtype=torch.float))\n",
    "    edge_index = edge_index.long()\n",
    "    x = torch.tensor(X, dtype=torch.float)\n",
    "    \n",
    "    # Include the label in the Data object\n",
    "    y = torch.tensor(label, dtype=torch.long)\n",
    "    \n",
    "    data = Data(x=x, edge_index=edge_index, y=y)\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_videos_parallel(video_paths, labels, T=48):\n",
    "    data_list = []\n",
    "    \n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = {executor.submit(process_video, video_path, label, T): (video_path, label) for video_path, label in zip(video_paths, labels)}\n",
    "        \n",
    "        # Use tqdm to display progress\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing Videos\"):\n",
    "            video_path, label = futures[future]\n",
    "            try:\n",
    "                data = future.result()  # The `Data` object returned from `process_video`\n",
    "                data_list.append(data)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing video {video_path}: {e}\")\n",
    "\n",
    "    return data_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed data to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the labels to integer indices\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Videos: 100%|██████████| 2494/2494 [2:13:09<00:00,  3.20s/it]  \n",
      "Processing Videos: 100%|██████████| 480/480 [25:47<00:00,  3.22s/it] \n"
     ]
    }
   ],
   "source": [
    "train_data = process_videos_parallel(X_train, y_train_encoded, 48)\n",
    "test_data = process_videos_parallel(X_test, y_test_encoded, 48)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, dropout_prob=0.5):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.fc = torch.nn.Linear(hidden_channels, out_channels)\n",
    "        self.dropout = torch.nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.bn2(x)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        # Apply global mean pooling to get a graph-level representation\n",
    "        x = global_mean_pool(x, batch)\n",
    "\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCNModelEnhanced(in_channels=3, hidden_channels=16, out_channels=20)  # Adjust output channels based on number of classes\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, criterion, device, scheduler=None, grad_clip=None):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "        output = model(data.x, data.edge_index, data.batch)  # Forward pass\n",
    "        loss = criterion(output, data.y)  # Calculate loss\n",
    "        loss.backward()  # Backward pass (compute gradients)\n",
    "        \n",
    "        # Gradient clipping\n",
    "        if grad_clip is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        \n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "        running_loss += loss.item() * data.num_graphs\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        correct += (predicted == data.y).sum().item()\n",
    "        total += data.y.size(0)\n",
    "    \n",
    "    # Step the learning rate scheduler, if provided\n",
    "    if scheduler is not None:\n",
    "        scheduler.step(running_loss)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    accuracy = correct / total\n",
    "    return epoch_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, criterion, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # No need to calculate gradients during evaluation\n",
    "        for data in test_loader:\n",
    "            data = data.to(device)\n",
    "\n",
    "            output = model(data.x, data.edge_index, data.batch)  # Forward pass\n",
    "            loss = criterion(output, data.y)  # Calculate loss\n",
    "\n",
    "            running_loss += loss.item() * data.num_graphs\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            correct += (predicted == data.y).sum().item()\n",
    "            total += data.y.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(test_loader.dataset)\n",
    "    accuracy = correct / total\n",
    "    return epoch_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\anaconda3\\envs\\KArsl-502\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "num_epochs = 10000\n",
    "grad_clip = 1.0  # Gradient clipping threshold\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 3.0018, Train Accuracy: 0.0449, Test Loss: 2.9980, Test Accuracy: 0.0500\n",
      "Epoch 2/10, Train Loss: 2.9968, Train Accuracy: 0.0529, Test Loss: 2.9977, Test Accuracy: 0.0500\n",
      "Epoch 3/10, Train Loss: 2.9964, Train Accuracy: 0.0445, Test Loss: 2.9979, Test Accuracy: 0.0500\n",
      "Epoch 4/10, Train Loss: 2.9960, Train Accuracy: 0.0389, Test Loss: 2.9982, Test Accuracy: 0.0500\n",
      "Epoch 5/10, Train Loss: 2.9960, Train Accuracy: 0.0473, Test Loss: 2.9983, Test Accuracy: 0.0500\n",
      "Epoch 6/10, Train Loss: 2.9964, Train Accuracy: 0.0465, Test Loss: 2.9983, Test Accuracy: 0.0500\n",
      "Epoch 7/10, Train Loss: 2.9966, Train Accuracy: 0.0425, Test Loss: 2.9980, Test Accuracy: 0.0500\n",
      "Epoch 8/10, Train Loss: 2.9961, Train Accuracy: 0.0409, Test Loss: 2.9978, Test Accuracy: 0.0500\n",
      "Epoch 9/10, Train Loss: 2.9964, Train Accuracy: 0.0433, Test Loss: 2.9984, Test Accuracy: 0.0500\n",
      "Epoch 10/10, Train Loss: 2.9959, Train Accuracy: 0.0477, Test Loss: 2.9986, Test Accuracy: 0.0500\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_accuracy = train(model, train_loader, optimizer, criterion, device, scheduler, grad_clip)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, '\n",
    "          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '\n",
    "          f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(train_data, \"Saved/train_data.pth\")\n",
    "torch.save(test_data, \"Saved/test_data.pth\")\n",
    "torch.save(model.state_dict(), \"Saved/gcn_model.pth\")\n",
    "with open(\"saved/label_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(label_encoder, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNModel1(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GCNModel1, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.fc = torch.nn.Linear(hidden_channels, out_channels)  # Final fully connected layer\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = global_mean_pool(x, batch)  # Pooling over all nodes in the graph\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Second model\n",
    "class GCNModel2(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, dropout_prob=0.5):\n",
    "        super(GCNModel2, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.fc = torch.nn.Linear(hidden_channels, out_channels)\n",
    "        self.dropout = torch.nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.bn2(x)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_13624\\4057385363.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model1.load_state_dict(torch.load('Saved/gcn_model.pth'))\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_13624\\4057385363.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model2.load_state_dict(torch.load('Saved/gcn_model_1.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Instantiate both models\n",
    "model1 = GCNModel1(in_channels=3, hidden_channels=16, out_channels=20).to(device)  # Adjust in_channels, out_channels as needed\n",
    "model2 = GCNModel2(in_channels=3, hidden_channels=16, out_channels=20, dropout_prob=0.5).to(device)\n",
    "\n",
    "# Load saved weights\n",
    "model1.load_state_dict(torch.load('Saved/gcn_model.pth'))\n",
    "model2.load_state_dict(torch.load('Saved/gcn_model_1.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_pipeline(video_path, model, label_encoder, T=48):\n",
    "    # Step 1: Preprocess the video\n",
    "    sampled_frames = sample_frames(video_path, T)\n",
    "    detected_landmarks = detect_landmarks(sampled_frames)\n",
    "    normalized_landmarks = normalize_landmarks(detected_landmarks)\n",
    "    graph, X = construct_graph(normalized_landmarks)\n",
    "    adj_matrix = get_adjacency_matrix(graph)\n",
    "    \n",
    "    edge_index, _ = dense_to_sparse(torch.tensor(adj_matrix, dtype=torch.float))\n",
    "    edge_index = edge_index.long()\n",
    "    x = torch.tensor(X, dtype=torch.float)\n",
    "    \n",
    "    # Step 2: Create a Data object for the video\n",
    "    data = Data(x=x, edge_index=edge_index)\n",
    "    \n",
    "    # Since this is a single example, we add a dummy batch dimension\n",
    "    data.batch = torch.zeros(data.num_nodes, dtype=torch.long)\n",
    "    \n",
    "    # Step 3: Perform inference\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():  # No need to compute gradients for inference\n",
    "        output = model(data.x, data.edge_index, data.batch)\n",
    "        predicted_class = output.argmax(dim=1).item()  # Get the class with highest score\n",
    "    \n",
    "    # Step 4: Decode the predicted class to the original label\n",
    "    predicted_label = label_encoder.inverse_transform([predicted_class])[0]\n",
    "    \n",
    "    return predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 - Loss: 0.8824, Accuracy: 0.7354\n",
      "Model 2 - Loss: 0.8457, Accuracy: 0.7354\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model1\n",
    "model1_loss, model1_accuracy = evaluate(model1, test_loader, criterion, device)\n",
    "print(f\"Model 1 - Loss: {model1_loss:.4f}, Accuracy: {model1_accuracy:.4f}\")\n",
    "\n",
    "# Evaluate model2\n",
    "model2_loss, model2_accuracy = evaluate(model2, test_loader, criterion, device)\n",
    "print(f\"Model 2 - Loss: {model2_loss:.4f}, Accuracy: {model2_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_on_videos(model, X_test, y_test_encoded, label_encoder, T=48, num_videos=100):\n",
    "    correct_predictions = 0\n",
    "    total_time = 0\n",
    "    total_videos = min(num_videos, len(X_test))  # Use the first 100 videos or fewer if X_test is smaller\n",
    "\n",
    "    for i in range(total_videos):\n",
    "        video_path = X_test[i]\n",
    "        true_label = y_test_encoded[i]\n",
    "\n",
    "        start_time = time.time()\n",
    "        predicted_label = inference_pipeline(video_path, model, label_encoder, T)\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Compare predicted label to the true label\n",
    "        if predicted_label == label_encoder.inverse_transform([true_label])[0]:\n",
    "            correct_predictions += 1\n",
    "\n",
    "        # Calculate time taken for this inference\n",
    "        total_time += (end_time - start_time)\n",
    "\n",
    "    # Calculate accuracy and average inference time\n",
    "    accuracy = correct_predictions / total_videos\n",
    "    average_time_per_inference = total_time / total_videos\n",
    "\n",
    "    return accuracy, average_time_per_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 - Accuracy: 0.7300, Average Inference Time: 5.2008 seconds\n",
      "Model 2 - Accuracy: 0.6800, Average Inference Time: 5.1972 seconds\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the first model\n",
    "model1_accuracy, model1_avg_time = evaluate_model_on_videos(model1, X_test, y_test_encoded, label_encoder, T=48, num_videos=100)\n",
    "print(f\"Model 1 - Accuracy: {model1_accuracy:.4f}, Average Inference Time: {model1_avg_time:.4f} seconds\")\n",
    "\n",
    "# Evaluate the second model\n",
    "model2_accuracy, model2_avg_time = evaluate_model_on_videos(model2, X_test, y_test_encoded, label_encoder, T=48, num_videos=100)\n",
    "print(f\"Model 2 - Accuracy: {model2_accuracy:.4f}, Average Inference Time: {model2_avg_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted label for the video is: يفكر\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "video_path = r\"Data/train/181/03_02_0181_(25_05_17_17_36_14)_c.mp4\"\n",
    "predicted_label = inference_pipeline(video_path, model, label_encoder)\n",
    "print(f\"The predicted label for the video is: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from torch_geometric.utils import add_self_loops, to_dense_adj\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score    \n",
    "from ruptures import Pelt\n",
    "from ruptures.metrics import hausdorff\n",
    "from ruptures.costs import CostL2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_9696\\448686073.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  train_data = torch.load(\"Saved/train_data.pth\")\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_9696\\448686073.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test_data = torch.load(\"Saved/test_data.pth\")\n"
     ]
    }
   ],
   "source": [
    "train_data = torch.load(\"Saved/train_data.pth\")\n",
    "test_data = torch.load(\"Saved/test_data.pth\")\n",
    "with open(\"Saved/label_encoder.pkl\", \"rb\") as f:\n",
    "    label_encoder = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_31536\\715594711.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('Saved/gcn_model.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GCNModel(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GCNModel, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.fc = torch.nn.Linear(hidden_channels, out_channels)  # Final fully connected layer\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = global_mean_pool(x, batch)  # Pooling over all nodes in the graph\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "model = GCNModel(in_channels=3, hidden_channels=16, out_channels=20)\n",
    "model.load_state_dict(torch.load('Saved/gcn_model.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sliding Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_change_points(video_path, model, label_encoder, min_size=5, penalty=10):\n",
    "    \"\"\"\n",
    "    Detects change points in a video and classifies segments.\n",
    "    \n",
    "    Parameters:\n",
    "    - video_path: Path to the input video file.\n",
    "    - model: The trained GCN model.\n",
    "    - label_encoder: The label encoder used to encode/decode class labels.\n",
    "    - min_size: Minimum segment size (in frames) for change point detection.\n",
    "    - penalty: Penalty value for change point detection sensitivity.\n",
    "    \n",
    "    Returns:\n",
    "    - A sentence representing the detected signs.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    N = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_features = []\n",
    "\n",
    "    # Feature extraction (e.g., pixel-wise differences between frames)\n",
    "    _, prev_frame = cap.read()\n",
    "    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "    for i in range(1, N):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        diff = cv2.absdiff(prev_gray, gray)\n",
    "        frame_features.append(np.mean(diff))\n",
    "        prev_gray = gray\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # Convert to numpy array\n",
    "    frame_features = np.array(frame_features)\n",
    "\n",
    "    # Change point detection using PELT\n",
    "    model_cp = Pelt(model=\"l2\", min_size=min_size).fit(frame_features)\n",
    "    change_points = model_cp.predict(pen=penalty)\n",
    "\n",
    "    # Add start and end points\n",
    "    change_points = [0] + change_points + [N]\n",
    "\n",
    "    predicted_labels = []\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    for start, end in zip(change_points[:-1], change_points[1:]):\n",
    "        sampled_frames = []\n",
    "        for i in range(start, end):\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                sampled_frames.append(frame)\n",
    "\n",
    "        if len(sampled_frames) < 2:\n",
    "            continue\n",
    "        \n",
    "        # Apply the existing inference pipeline to each segment\n",
    "        detected_landmarks = detect_landmarks(sampled_frames)\n",
    "        normalized_landmarks = normalize_landmarks(detected_landmarks)\n",
    "        graph, X = construct_graph(normalized_landmarks)\n",
    "        adj_matrix = get_adjacency_matrix(graph)\n",
    "        \n",
    "        edge_index, _ = dense_to_sparse(torch.tensor(adj_matrix, dtype=torch.float))\n",
    "        edge_index = edge_index.long()\n",
    "        x = torch.tensor(X, dtype=torch.float)\n",
    "        \n",
    "        data = Data(x=x, edge_index=edge_index)\n",
    "        data.batch = torch.zeros(data.num_nodes, dtype=torch.long)\n",
    "\n",
    "        # Perform inference on the segment\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = model(data.x, data.edge_index, data.batch)\n",
    "            predicted_class = output.argmax(dim=1).item()\n",
    "\n",
    "        # Decode the predicted class to the original label\n",
    "        predicted_label = label_encoder.inverse_transform([predicted_class])[0]\n",
    "        predicted_labels.append(predicted_label)\n",
    "    \n",
    "    cap.release()\n",
    "\n",
    "    # Join the final labels into a sentence\n",
    "    sentence = \" \".join(predicted_labels)\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sentence: يبني يبني يمشي\n"
     ]
    }
   ],
   "source": [
    "video_path = r\"C:\\Users\\LENOVO\\Downloads\\03-03-0171-21-03-17-22-04-56-c_CnFcgmB0.mp4\"\n",
    "predicted_sentence = detect_change_points(video_path, model, label_encoder)\n",
    "print(\"Predicted Sentence:\", predicted_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KArsl-502",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
